{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn as nn\n",
    "import math\n",
    "\n",
    "class LunaModel(nn.Module):\n",
    "    def __init__(self, in_channels = 1, conv_channels = 8):\n",
    "        super().__init__()\n",
    "\n",
    "        self.tail_batchnorm = nn.BatchNorm3d(1)\n",
    "\n",
    "        self.block1 = LunaBlock(in_channels , conv_channels)\n",
    "        self.block2 = LunaBlock(conv_channels , conv_channels * 2)\n",
    "        self.block3 = LunaBlock(conv_channels * 2, conv_channels * 4)\n",
    "        self.block4 = LunaBlock(conv_channels * 4 , conv_channels * 8)\n",
    "\n",
    "        self.head_linear = nn.Linear(1152 , 2)\n",
    "        self.head_softmax = nn.Softmzx(dim = 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self,input_batch):\n",
    "        bn_output = self.tail_batchnorm(input_batch)\n",
    "\n",
    "        block_out = self.block1(bn_output)\n",
    "        block_out = self.block2(block_out)\n",
    "        block_out = self.block3(block_out)\n",
    "        block_out = self.block4(block_out)\n",
    "\n",
    "        conv_flat = block_out.view(\n",
    "            block_out.size(0),\n",
    "            -1,\n",
    "        )\n",
    "\n",
    "        linear_output = self.head_linear(conv_flat)\n",
    "\n",
    "        return linear_output , self.head_softmax(linear_output)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # 여러 가지 정규화 기술로 계층의 출력을 잘 동작하게 만들 수도 있지만 가장 단순한 방법은\n",
    "        # 단지 중간 값이나 기울기가 이유 없이 작아지거나 커지지 않도록 신경망 가중치가 확실히 초기화되게 하는 것이다.\n",
    "\n",
    "        for m in self.modules():\n",
    "            if type(m) in {\n",
    "                nn.Linear,\n",
    "                nn.Conv3d,\n",
    "                nn.Conv2d,\n",
    "                nn.ConvTranspose2d,\n",
    "                nn.ConvTranspose3d,\n",
    "            }:\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight.data, a=0, mode='fan_out', nonlinearity='relu',\n",
    "                )\n",
    "                if m.bias is not None:\n",
    "                    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(m.weight.data)\n",
    "                    bound = 1 / math.sqrt(fan_out)\n",
    "                    nn.init.normal_(m.bias, -bound , bound)\n",
    "        \n",
    "\n",
    "class LunaBlock(nn.Module):\n",
    "    def __init__(self, in_channels, conv_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv3d(\n",
    "            in_channels , conv_channels, kernel_size= 3, padding = 1, bias = True,\n",
    "        )\n",
    "        self.relu1 = nn.ReLU(inplace = True)\n",
    "        # inplace = True 를 사용할 때 좋은 점은 입력 텐서의 메모리를 직접 수정하여 연산속도를 향상시키는 효과가 있다.\n",
    "        # 주의할 점은 원본 텐서가 수정되기 때문에, 연산 이후 원본 텐서를 사용하는 다른 연산에 영향을 줄 수 있다는 점이다.\n",
    "        self.conv2 = nn.Conv3d(\n",
    "            conv_channels, conv_channels, kernel_size = 3, padding = 1, bias = True,\n",
    "        )\n",
    "        self.relu2 = nn.ReLU(inplace = True)\n",
    "\n",
    "        self.maxpool = nn.MaxPool3d(2,2)\n",
    "\n",
    "    def forward(self, input_batch):\n",
    "        block_out = self.conv1(input_batch)\n",
    "        block_out = self.relu1(block_out)\n",
    "        block_out = self.conv2(block_out)\n",
    "        block_out = self.relu2(block_out)\n",
    "\n",
    "        return self.maxpool(block_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## logger 정의ㅣ\n",
    "import logging\n",
    "import logging.handlers\n",
    "\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "## logger level 정의 : INFO level\n",
    "root_logger.setLevel(logging.INFO)\n",
    "\n",
    "# Some libraries attempt to add their own root logger handlers. This is\n",
    "# annoying and so we get rid of them.\n",
    "## 기존 logger 삭제\n",
    "for handler in list(root_logger.handlers):\n",
    "    root_logger.removeHandler(handler)\n",
    "\n",
    "## logger handler 포맷 설증\n",
    "logfmt_str = \"%(asctime)s %(levelname)-8s pid:%(process)d %(name)s:%(lineno)03d:%(funcName)s %(message)s\"\n",
    "formatter = logging.Formatter(logfmt_str)\n",
    "\n",
    "streamHandler = logging.StreamHandler()\n",
    "streamHandler.setFormatter(formatter)\n",
    "streamHandler.setLevel(logging.DEBUG)\n",
    "\n",
    "root_logger.addHandler(streamHandler)\n",
    "\n",
    "## log의 handler 를 설정해 주어야 메시지를 출력할 수 있다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (574108209.py, line 95)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 95\u001b[0;36m\u001b[0m\n\u001b[0;31m    batch_size = batch_size\u001b[0m\n\u001b[0m                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import datetime\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import SGD, Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "log = logging.getLogger(__name__)\n",
    "log.setLevel(logging.DEBUG)\n",
    "\n",
    "class LUNATrainingApp:\n",
    "    def __init__(self, sys_argv = None):\n",
    "        if sys_argv is None:\n",
    "            sys_argv = sys.argv[1:]\n",
    "        \n",
    "        parser = argparse.ArgumentParser()\n",
    "        parser.add_argument('--num-workers',\n",
    "                            default = 8,\n",
    "                            type = int,\n",
    "        )\n",
    "\n",
    "        parser.add_argument('--batch-size',\n",
    "                            default= 32,\n",
    "                            type=int,\n",
    "                            )\n",
    "        \n",
    "        parser.add_argument('--tb-prefix',\n",
    "                            default = 'PYTORCH-MASTER',\n",
    "                            )\n",
    "        \n",
    "        self.cli_args = parser.parse_args(sys_argv)\n",
    "        self.time_str = datetime.datetime.now().strftime('%Y-%m-%d_%H.%M.%S')\n",
    "\n",
    "        self.trn_writer = None\n",
    "        self.val_writer = None\n",
    "        self.totalTrainingSamples_count = 0\n",
    "\n",
    "        self.use_cuda = torch.backends.mps.is_available()\n",
    "        self.device = torch.device('mps' if self.use_cuda else 'cpu')\n",
    "\n",
    "        self.model = self.initModel()\n",
    "        self.optimizer = self.initOptimizer()\n",
    "\n",
    "    def initModel(self):\n",
    "        model = LunaModel()\n",
    "        if self.use_cuda:\n",
    "            log.info('Using CUDA; {} devices.'.format(torch.cuda.device_count()))\n",
    "            if torch.cuda.device_count() > 1:\n",
    "                model = nn.DataParallel(model)\n",
    "            model = model.to(self.device)\n",
    "        return model\n",
    "    \n",
    "    def initOptimizer(self):\n",
    "        return SGD(self.model.parameters(), lr=0.001, momentum=0.99)\n",
    "\n",
    "    def initTrainDl(self):\n",
    "        train_ds = LunaDataset(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=False\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "        \n",
    "        train_dl = DataLoader(\n",
    "            train_ds,\n",
    "            batch_size = batch_size,\n",
    "            num_workers = self.cli_args.num_workers,\n",
    "            pin_memory = self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return train_dl\n",
    "\n",
    "    def initValDl(self):\n",
    "        val_ds = LunaDatast(\n",
    "            val_stride=10,\n",
    "            isValSet_bool=True,\n",
    "        )\n",
    "\n",
    "        batch_size = self.cli_args.batch_size\n",
    "        if self.use_cuda:\n",
    "            batch_size *= torch.cuda.device_count()\n",
    "\n",
    "        val_dl = DataLoader(\n",
    "            val_ds,\n",
    "            batch_size = batch_size\n",
    "            num_workers=self.cli_args.num_workers,\n",
    "            pin_memory=self.use_cuda,\n",
    "        )\n",
    "\n",
    "        return val_dl\n",
    "    \n",
    "    def main(self):\n",
    "        log.info(\"Starting {} {}\".format(type(self).__name__, self.cli_args))\n",
    "\n",
    "        train_dl = self.initTrainDl()\n",
    "        val_dl = self.initValDl()\n",
    "\n",
    "        for epoch_ndx in range(1, self.cli_args.epochs + 1):\n",
    "            log.info(\"Epoch {} of {}, {}/{} batches of size {}*{}\".format(\n",
    "                epoch_ndx,\n",
    "                self.cli_args.epochs,\n",
    "                len(train_dl),\n",
    "                len(val_dl),\n",
    "                self.cli_args.batch_size,\n",
    "                (torch.cuda.device_count() if self.use_cuda else 1),\n",
    "            ))\n",
    "\n",
    "            trnMetrics_t = self.doTraining(epoch_ndx, train_dl)\n",
    "            self.logMetrics(epoch_ndx, 'trn', trnMetrics_t)\n",
    "\n",
    "            valMetrics_t = self.doValidation(epoch_ndx, val_dl)\n",
    "            self.logMetrics(epoch_ndx, 'val', valMetrics_t)\n",
    "\n",
    "    def doTraining(self, epoch_ndx, train_dl):\n",
    "        self.model.train()\n",
    "        trnMetrics_g = torch.zeros(\n",
    "            METRICS_SIZE,\n",
    "            len(train_dl.dataset),\n",
    "            device = self.device,\n",
    "        )\n",
    "\n",
    "        batch_iter = enumerateWithEstimate(\n",
    "            train_dl,\n",
    "            \"E{} Training\".format(epoch_ndx),\n",
    "            start_ndx = train_dl.num_workers,\n",
    "        )\n",
    "        for batch_ndx, batch_tup in batch_iter:\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            loss_var = self.computeBatchLoss(\n",
    "                batch_ndx,\n",
    "                batch_tup,\n",
    "                train_dl.batch_size,\n",
    "                trnMetrics_g\n",
    "            )\n",
    "\n",
    "            loss_var.backward()\n",
    "            self.optimizer.step()\n",
    "            \n",
    "        self.totalTrainingSamples_count += len(train_dl.dataset)\n",
    "\n",
    "        return trnMetrics_g.to('cpu')\n",
    "\n",
    "    def doValidation(self, epoch_ndx, val_dl):\n",
    "        with torch.no_grad():\n",
    "            self.model.eval()\n",
    "            valMetrics_g = torch.zeros(\n",
    "                METRICS_SIZE,\n",
    "                len(val_dl.dataset),\n",
    "                device = self.device\n",
    "            )\n",
    "\n",
    "            batch_iter = enumerateWithEstimate(\n",
    "                val_dl,\n",
    "                \"E{} Validation\".format(epoch_ndx),\n",
    "                start_ndx = val_dl.num_workers,\n",
    "            )\n",
    "\n",
    "            for batch_ndx, batch_tup in batch_iter:\n",
    "                self.computeBatchLoss(\n",
    "                    batch_ndx, batch_tup, val_dl.batch_size, valMetrics_g\n",
    "                )\n",
    "\n",
    "        return valMetrics_g.to('cpu')\n",
    "\n",
    "    \n",
    "    def computeBatchLoss(self, batch_ndx, batch_tup, batch_size, metrics_g):\n",
    "        input_t, label_t, _series_list, _center_list = batch_tup\n",
    "\n",
    "        input_g = intpu_t.to(self.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_master",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
